{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 09:25:42.809937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "csv_file = '../resources/esb1_preprocessed.csv'\n",
    "model_dir = '../resources/mlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ESB_Tower_1 leavingWaterTemp', 'Cell1_Fan vfdPercent',\n",
       "       'ESB_Tower_1 enteringWaterTemp', 'ESB_Tower_1 bypassValveOpenClose',\n",
       "       'ESB_Tower_1 outdoorAirDryBulb', 'ESB_Tower_1 outdoorAirHumidity',\n",
       "       'ESB_Tower_1 outdoorAirWetBulb', 'DayOfWeek', 'HourOfDay',\n",
       "       'Setpoint_Python'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['ESB_Tower_1 leavingWaterTemp', 'Cell1_Fan vfdPercent', \n",
    "       'ESB_Tower_1 enteringWaterTemp', 'ESB_Tower_1 bypassValveOpenClose',\n",
    "       'ESB_Tower_1 outdoorAirDryBulb', 'ESB_Tower_1 outdoorAirHumidity',\n",
    "       'ESB_Tower_1 outdoorAirWetBulb', 'DayOfWeek', 'HourOfDay',\n",
    "       'Setpoint_Python', ]\n",
    "df = pd.read_csv(csv_file, index_col='time')[columns]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HVACEnvironment(gym.Env):\n",
    "    def __init__(self, csv_file, model_file):\n",
    "        super(HVACEnvironment, self).__init__()\n",
    "\n",
    "        columns = ['ESB_Tower_1 leavingWaterTemp', 'Cell1_Fan vfdPercent', \n",
    "            'ESB_Tower_1 enteringWaterTemp', 'ESB_Tower_1 bypassValveOpenClose',\n",
    "            'ESB_Tower_1 outdoorAirDryBulb', 'ESB_Tower_1 outdoorAirHumidity',\n",
    "            'ESB_Tower_1 outdoorAirWetBulb', 'DayOfWeek', 'HourOfDay',\n",
    "            'Setpoint_Python', ]\n",
    "        \n",
    "        # Load data from CSV\n",
    "        self.data = pd.read_csv(csv_file, index_col='time')[columns]\n",
    "        \n",
    "        # Define observation space (features)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(9,), dtype=np.float32)\n",
    "        \n",
    "        # Define action space (python_setpoint)\n",
    "        # actions we can take: decrease, maintain, increase by 1\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = load_model(model_file)\n",
    "        \n",
    "        # Initial state\n",
    "        self.state = self.data.iloc[0].values\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action 0: self.state += (0-1) --> -1\n",
    "        action 1: self.state += (1-1) --> +0\n",
    "        action 2: self.state += (2-1) --> +1\n",
    "        \"\"\"\n",
    "        # Apply action (update python_setpoint)\n",
    "        python_setpoint = action\n",
    "\n",
    "        fanVFD = self.state[1]\n",
    "        \n",
    "        # Adjust fanVFD proportionally\n",
    "        # Assuming fanVFD is inversely proportional to python_setpoint\n",
    "        fanVFD = fanVFD - action / 100  # Assuming 100 is the maximum value of python_setpoint\n",
    "        \n",
    "        # Update state\n",
    "        self.state[2] = fanVFD\n",
    "        self.state[-1] = python_setpoint\n",
    "        \n",
    "        # Predict LeavingWaterTemp using the model\n",
    "        inp = np.asarray(self.state[1:]).reshape(1, 9).astype('float32')\n",
    "        print(inp)\n",
    "        leaving_water_temp = self.model.predict(inp)\n",
    "        \n",
    "        # Compute reward (you need to define your own reward function)\n",
    "        reward = self._calculate_reward(leaving_water_temp, fanVFD)\n",
    "        \n",
    "        # Check if termination condition is met (optional)\n",
    "        done = False  # Define your termination condition\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset environment to initial state\n",
    "        self.state = self.data.iloc[0].values\n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        # Optionally, add rendering functionality\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        # Clean up resources\n",
    "        pass\n",
    "    \n",
    "    def _calculate_reward(self, leaving_water_temp, fanVFD):\n",
    "        # Define your reward function\n",
    "\n",
    "        # Define target values\n",
    "        target_leaving_water_temp = 0  # Adjust as needed\n",
    "        target_fanVFD = 0  # Adjust as needed\n",
    "    \n",
    "        # Define constants\n",
    "        leaving_temp_weight = 0.7  # Weight for leaving water temperature in the reward calculation\n",
    "        perfreq_weight = 0.3  # Weight for fanVFD in the reward calculation\n",
    "        \n",
    "        # Compute reward based on leaving water temperature\n",
    "        leaving_temp_penalty = abs(leaving_water_temp - target_leaving_water_temp)\n",
    "        leaving_temp_reward = max(0, 1 - leaving_temp_penalty)  # Linearly decreasing reward with increasing deviation\n",
    "        \n",
    "        # Compute reward based on fanVFD\n",
    "        perfreq_penalty = abs(fanVFD - target_fanVFD)\n",
    "        perfreq_reward = max(0, 1 - perfreq_penalty)  # Linearly decreasing reward with increasing deviation\n",
    "        \n",
    "        # Combine the rewards with weights\n",
    "        total_reward = leaving_temp_weight * leaving_temp_reward + perfreq_weight * perfreq_reward\n",
    "\n",
    "        print(total_reward)\n",
    "        \n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 09:26:01.569715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "0.3\n",
      "Step 0: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.294\n",
      "Step 1: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.3\n",
      "Step 2: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 3: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.3\n",
      "Step 4: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.3\n",
      "Step 5: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 6: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 7: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.294\n",
      "Step 8: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.297\n",
      "Step 9: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 10: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.3\n",
      "Step 11: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.297\n",
      "Step 12: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.297\n",
      "Step 13: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.294\n",
      "Step 14: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.297\n",
      "Step 15: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "0.3\n",
      "Step 16: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.297\n",
      "Step 17: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.3\n",
      "Step 18: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.294\n",
      "Step 19: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 20: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 21: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.3\n",
      "Step 22: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 23: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.3\n",
      "Step 24: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 25: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "0.3\n",
      "Step 26: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.3\n",
      "Step 27: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 28: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.3\n",
      "Step 29: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.297\n",
      "Step 30: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "0.297\n",
      "Step 31: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.3\n",
      "Step 32: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "0.3\n",
      "Step 33: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.297\n",
      "Step 34: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "0.3\n",
      "Step 35: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.3\n",
      "Step 36: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 37: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 38: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 39: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 40: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.3\n",
      "Step 41: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.294\n",
      "Step 42: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "0.3\n",
      "Step 43: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "0.294\n",
      "Step 44: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.297\n",
      "Step 45: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 46: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.3\n",
      "Step 47: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 48: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.294\n",
      "Step 49: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.294\n",
      "Step 50: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.3\n",
      "Step 51: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.3\n",
      "Step 52: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.297\n",
      "Step 53: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.3\n",
      "Step 54: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 55: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 56: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 57: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.294\n",
      "Step 58: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 59: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.3\n",
      "Step 60: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.297\n",
      "Step 61: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.3\n",
      "Step 62: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "0.294\n",
      "Step 63: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 64: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 65: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.297\n",
      "Step 66: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.3\n",
      "Step 67: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 68: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 69: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.3\n",
      "Step 70: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 71: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 72: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.297\n",
      "Step 73: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.297\n",
      "Step 74: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 75: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.3\n",
      "Step 76: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.297\n",
      "Step 77: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 78: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.3\n",
      "Step 79: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.297\n",
      "Step 80: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.3\n",
      "Step 81: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.297\n",
      "Step 82: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 83: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 84: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "0.297\n",
      "Step 85: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.3\n",
      "Step 86: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 87: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.3\n",
      "Step 88: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.294\n",
      "Step 89: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 90: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.297\n",
      "Step 91: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.297\n",
      "Step 92: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 93: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.294\n",
      "Step 94: Action taken: 2, Reward: 0.294, Done: False\n",
      "[[ 0.        0.        0.       84.522064 51.04264  70.414894  2.\n",
      "   2.        0.      ]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.3\n",
      "Step 95: Action taken: 0, Reward: 0.3, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.297\n",
      "Step 96: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.297\n",
      "Step 97: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -9.9999998e-03  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   1.0000000e+00]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.297\n",
      "Step 98: Action taken: 1, Reward: 0.297, Done: False\n",
      "[[ 0.0000000e+00 -2.0000000e-02  0.0000000e+00  8.4522064e+01\n",
      "   5.1042641e+01  7.0414894e+01  2.0000000e+00  2.0000000e+00\n",
      "   2.0000000e+00]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "0.294\n",
      "Step 99: Action taken: 2, Reward: 0.294, Done: False\n",
      "Total reward accumulated: 29.691000000000038\n"
     ]
    }
   ],
   "source": [
    "# Example usage with random actions\n",
    "env = HVACEnvironment(csv_file=csv_file, model_file=model_dir)\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(100):  # Set a maximum number of steps\n",
    "    action = env.action_space.sample()  # Take a random action\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Step {step}: Action taken: {action}, Reward: {reward}, Done: {done}\")\n",
    "    \n",
    "    if done:\n",
    "        print(\"Environment reached termination condition.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total reward accumulated: {total_reward}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
